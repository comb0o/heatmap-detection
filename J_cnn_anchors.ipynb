{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab2a6f9a-fb09-4b5a-96d4-561fc75c5250",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T16:01:02.808808Z",
     "iopub.status.busy": "2025-08-07T16:01:02.808502Z",
     "iopub.status.idle": "2025-08-07T16:01:04.064321Z",
     "shell.execute_reply": "2025-08-07T16:01:04.063824Z",
     "shell.execute_reply.started": "2025-08-07T16:01:02.808790Z"
    }
   },
   "outputs": [],
   "source": [
    "#%pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2f75909-3dd2-4392-90e7-f36aaec17957",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T16:01:04.065034Z",
     "iopub.status.busy": "2025-08-07T16:01:04.064858Z",
     "iopub.status.idle": "2025-08-07T16:01:04.067861Z",
     "shell.execute_reply": "2025-08-07T16:01:04.067444Z",
     "shell.execute_reply.started": "2025-08-07T16:01:04.065016Z"
    }
   },
   "outputs": [],
   "source": [
    "def sprint(**kwargs):\n",
    "    for name, val in kwargs.items():\n",
    "        print(f\"{name}: {val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbe2b0d1-3144-4f4e-8663-7706a40c9bc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T16:01:04.068578Z",
     "iopub.status.busy": "2025-08-07T16:01:04.068383Z",
     "iopub.status.idle": "2025-08-07T16:01:06.597851Z",
     "shell.execute_reply": "2025-08-07T16:01:06.597381Z",
     "shell.execute_reply.started": "2025-08-07T16:01:04.068564Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as TF\n",
    "from functools import lru_cache\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MAX_BOXES = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ad89302-f378-4af8-9434-eb605e836e1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T16:01:06.605831Z",
     "iopub.status.busy": "2025-08-07T16:01:06.605623Z",
     "iopub.status.idle": "2025-08-07T16:01:06.618972Z",
     "shell.execute_reply": "2025-08-07T16:01:06.618528Z",
     "shell.execute_reply.started": "2025-08-07T16:01:06.605815Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_dataset(img_dir, label_dir, train_ratio=0.7, val_ratio=0.25, test_ratio=0.05, seed=42):\n",
    "    random.seed(seed)\n",
    "\n",
    "    IMG_EXTS = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}\n",
    "    LAB_EXT = '.txt'\n",
    "\n",
    "    image_files = [f for f in sorted(os.listdir(img_dir)) if os.path.splitext(f)[1].lower() in IMG_EXTS]\n",
    "    label_files = {f for f in os.listdir(label_dir) if f.lower().endswith(LAB_EXT)}\n",
    "\n",
    "    pairs = []\n",
    "    for img in image_files:\n",
    "        base, _ = os.path.splitext(img)\n",
    "        lbl = base + LAB_EXT\n",
    "        if lbl in label_files:\n",
    "            pairs.append((img, lbl))\n",
    "\n",
    "    random.shuffle(pairs)\n",
    "    total = len(pairs)\n",
    "    n_train = int(total * train_ratio)\n",
    "    n_val = int(total * val_ratio)\n",
    "\n",
    "    train_pairs = pairs[:n_train]\n",
    "    val_pairs = pairs[n_train:n_train + n_val]\n",
    "    test_pairs = pairs[n_train + n_val:]\n",
    "\n",
    "    return train_pairs, val_pairs, test_pairs\n",
    "\n",
    "\n",
    "def collate_fixed_boxes(batch, max_boxes=MAX_BOXES):\n",
    "    images, targets = zip(*batch)\n",
    "    images = torch.stack(images, dim=0)\n",
    "\n",
    "    batch_size = len(targets)\n",
    "\n",
    "    l_padded = torch.zeros((batch_size, max_boxes, 4), dtype=torch.float32)\n",
    "    s_padded = torch.zeros((batch_size, max_boxes, 4), dtype=torch.float32)\n",
    "    for i, target in enumerate(targets):\n",
    "        l_b = target[\"l_boxes\"]\n",
    "        l_n = l_b.size(0)\n",
    "        l_padded[i, :l_n] = l_b[:max_boxes]\n",
    "\n",
    "        s_b = target[\"s_boxes\"]\n",
    "        s_n = s_b.size(0)\n",
    "        s_padded[i, :s_n] = s_b[:max_boxes]\n",
    "    \n",
    "    return images, {\"l_boxes\" : l_padded, \"s_boxes\" : s_padded}\n",
    "\n",
    "\n",
    "def resize_image_and_boxes(image, boxes, size):\n",
    "    \"\"\"\n",
    "    image: Tensor[C, H, W]\n",
    "    boxes: Tensor[N, 4] in (x_min, y_min, x_max, y_max)\n",
    "    size:   (new_h, new_w)\n",
    "    \"\"\"\n",
    "    _, orig_h, orig_w = image.shape\n",
    "    new_h, new_w = size\n",
    "\n",
    "    # Resize image\n",
    "    image = TF.resize(image, [new_h, new_w])\n",
    "\n",
    "    # Compute scale factors\n",
    "    scale_x = new_w / orig_w\n",
    "    scale_y = new_h / orig_h\n",
    "\n",
    "    # Adjust boxes\n",
    "    boxes = boxes.clone()\n",
    "    boxes[:, [0, 2]] = boxes[:, [0, 2]] * scale_x\n",
    "    boxes[:, [1, 3]] = boxes[:, [1, 3]] * scale_y\n",
    "\n",
    "    boxes[:, [4, 6]] = boxes[:, [4, 6]] * scale_x\n",
    "    boxes[:, [5, 7]] = boxes[:, [5, 7]] * scale_y\n",
    "\n",
    "    return image, boxes\n",
    "\n",
    "\n",
    "class DualCompose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, boxes):\n",
    "        for t in self.transforms:\n",
    "            image, boxes = t(image, boxes)\n",
    "        return image, boxes\n",
    "\n",
    "\n",
    "class RandomFlip:\n",
    "    def __init__(self, p_h=0.5, p_v=0.5):\n",
    "        self.p_h = p_h\n",
    "        self.p_v = p_v\n",
    "\n",
    "    def __call__(self, tensor, boxes):\n",
    "        _, h, w = tensor.shape\n",
    "        flipped = False\n",
    "\n",
    "        # Horizontal flip\n",
    "        if random.random() < self.p_h:\n",
    "            tensor = TF.hflip(tensor)\n",
    "            boxes = boxes.clone()\n",
    "            boxes[:, [0, 2]] = w - boxes[:, [2, 0]]  # flip x_min and x_max\n",
    "            boxes[:, [4, 6]] = w - boxes[:, [6, 4]]  # flip x_min and x_max\n",
    "            flipped = True\n",
    "    \n",
    "        # Vertical flip\n",
    "        if random.random() < self.p_v:\n",
    "            tensor = TF.vflip(tensor)\n",
    "            boxes = boxes.clone()\n",
    "            boxes[:, [1, 3]] = h - boxes[:, [3, 1]]  # flip y_min and y_max\n",
    "            boxes[:, [5, 7]] = h - boxes[:, [7, 5]]  # flip y_min and y_max\n",
    "            flipped = True\n",
    "    \n",
    "        return tensor, boxes\n",
    "\n",
    "\n",
    "class AddGaussianNoise:\n",
    "    def __init__(self, mean=0., std=0.01):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor, boxes):\n",
    "        noise = torch.randn_like(tensor) * self.std + self.mean\n",
    "        tensor = torch.clamp(tensor + noise, 0.0, 1.0)\n",
    "        \n",
    "        return tensor, boxes\n",
    "\n",
    "\n",
    "class SyntheticDataset(Dataset):\n",
    "    def __init__(self, file_pairs, img_dir, label_dir, transform=None, resize=None, test=False):\n",
    "        self.image_files, self.label_files = zip(*file_pairs)\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.resize = resize\n",
    "        self.test = test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.image_files[idx])\n",
    "        if not os.path.isfile(img_path):\n",
    "            raise FileNotFoundError(f\"Image file not found: {img_path}\")\n",
    "            \n",
    "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        if image is None:\n",
    "            raise RuntimeError(f\"OpenCV failed to load image: {img_path}\")\n",
    "            \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        label_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "        if not os.path.isfile(label_path):\n",
    "            raise FileNotFoundError(f\"Label file not found: {label_path}\")\n",
    "            \n",
    "        boxes = []\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                box = list(map(int, line.strip().split()))\n",
    "                boxes.append(box)\n",
    "                \n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # Convert to PIL for augmentation if test mode is active\n",
    "        if self.test:\n",
    "            pil_image = Image.fromarray(image)\n",
    "            pil_image = add_random_elements(pil_image, num_elements=3)\n",
    "            image = np.array(pil_image)\n",
    "\n",
    "        # Convert to tensor\n",
    "        image = TF.to_tensor(image)\n",
    "        image = TF.normalize(image, mean=[0.5]*3, std=[0.5]*3)\n",
    "\n",
    "        if self.resize is not None:\n",
    "            image, boxes = resize_image_and_boxes(image, boxes, self.resize)\n",
    "\n",
    "        _, H, W = image.shape\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image, boxes = self.transform(image, boxes)\n",
    "\n",
    "        # normalize xyxy to [0,1]\n",
    "        # boxes[:, [0,2]] /= H    # x_min, x_max\n",
    "        # boxes[:, [1,3]] /= W    # y_min, y_max\n",
    "        \n",
    "        # now boxes[i] is in [0,1], return these as your GT\n",
    "        target = {\"s_boxes\" : boxes[:,:4]}\n",
    "        target.update({\"l_boxes\" : boxes[:,4:]})\n",
    "        \n",
    "        return image, target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f51f451-65f4-45a3-8dfb-3e141a5a32ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T16:01:06.621578Z",
     "iopub.status.busy": "2025-08-07T16:01:06.621356Z",
     "iopub.status.idle": "2025-08-07T16:01:06.627818Z",
     "shell.execute_reply": "2025-08-07T16:01:06.627410Z",
     "shell.execute_reply.started": "2025-08-07T16:01:06.621562Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_random_elements(image, num_elements=10):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    w, h = image.size\n",
    "\n",
    "    for _ in range(num_elements):\n",
    "        shape_type = random.choice(['circle', 'rectangle'])\n",
    "        color = tuple(random.randint(0, 255) for _ in range(3))\n",
    "        radius = random.randint(10, 20)\n",
    "\n",
    "        if shape_type == 'circle':\n",
    "            x = random.randint(radius, w - radius)\n",
    "            y = random.randint(radius, h - radius)\n",
    "            draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill=color)\n",
    "\n",
    "        elif shape_type == 'rectangle':\n",
    "            x1 = random.randint(0, w - radius)\n",
    "            y1 = random.randint(0, h - radius)\n",
    "            x2 = x1 + random.randint(10, radius)\n",
    "            y2 = y1 + random.randint(10, radius)\n",
    "            draw.rectangle((x1, y1, x2, y2), fill=color)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def show_boxes_with_confidence(image_tensor, l_gt_boxes=None, s_gt_boxes=None, pred_boxes=None, confidences=None, figsize=(8,8), box_kwargs=None, text_kwargs=None):\n",
    "    \"\"\"\n",
    "    Draw GT (red) and predicted (blue) boxes.  \n",
    "    If confidences is provided, writes 'c=0.83' at top-left of each pred box.\n",
    "    \"\"\"\n",
    "\n",
    "    # defaults\n",
    "    box_kwargs = box_kwargs or {}\n",
    "    text_kwargs = text_kwargs or dict(color='white', fontsize=10, bbox=dict(facecolor='blue', alpha=0.5, pad=0.5))\n",
    "\n",
    "    # prep image\n",
    "    img = image_tensor.permute(1,2,0).numpy()\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    ax = plt.gca()\n",
    "\n",
    "    def _draw_box(box, edgecolor):\n",
    "        x1,y1,x2,y2 = box\n",
    "        w,h = x2-x1, y2-y1\n",
    "        rect = Rectangle((x1,y1), w, h, linewidth=2, edgecolor=edgecolor, facecolor='none', **box_kwargs)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    # draw GT in red\n",
    "    if l_gt_boxes is not None:\n",
    "        for box in l_gt_boxes.tolist():\n",
    "            _draw_box(box, edgecolor='g')\n",
    "\n",
    "    if s_gt_boxes is not None:\n",
    "        for box in s_gt_boxes.tolist():\n",
    "            _draw_box(box, edgecolor='r')\n",
    "\n",
    "    # draw preds in blue + text\n",
    "    if pred_boxes is not None:\n",
    "        for i, (box, conf) in enumerate(zip(pred_boxes, confidences)):\n",
    "            if conf >= 0.5:\n",
    "                _draw_box(box, edgecolor='b')\n",
    "                if confidences is not None:\n",
    "                    x1,y1,_,_ = box\n",
    "                    ax.text(x1, y1 - 2, f\"{confidences[i]:.2f}\", **text_kwargs)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "354d0232-cdc2-4c47-b713-38d6bd54bfbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T16:01:06.628644Z",
     "iopub.status.busy": "2025-08-07T16:01:06.628299Z",
     "iopub.status.idle": "2025-08-07T16:01:06.653400Z",
     "shell.execute_reply": "2025-08-07T16:01:06.653007Z",
     "shell.execute_reply.started": "2025-08-07T16:01:06.628628Z"
    }
   },
   "outputs": [],
   "source": [
    "IMAGE_DIR = 'images'\n",
    "LABEL_DIR = 'labels'\n",
    "HEATMAP_DIR = 'heatmaps_rgb'\n",
    "IMAGE_SIZE = (256, 256)\n",
    "BATCH_SIZE = 32\n",
    "NUM_IMAGES = 5000\n",
    "OUTPUT_DIR = 'data/hm_dataset_min_max'\n",
    "\n",
    "\n",
    "train_data, valid_data, test_data = split_dataset(\n",
    "    img_dir = os.path.join(OUTPUT_DIR,IMAGE_DIR),\n",
    "    label_dir = os.path.join(OUTPUT_DIR,LABEL_DIR)\n",
    ")\n",
    "\n",
    "transform = DualCompose([\n",
    "    RandomFlip(p_h=0.5, p_v=0.5),\n",
    "    # AddGaussianNoise(mean=0., std=0.05)\n",
    "])\n",
    "\n",
    "train_dataset = SyntheticDataset(\n",
    "    train_data, os.path.join(OUTPUT_DIR,IMAGE_DIR), os.path.join(OUTPUT_DIR,LABEL_DIR),\n",
    "    transform = transform,\n",
    "    resize = IMAGE_SIZE\n",
    ")\n",
    "valid_dataset = SyntheticDataset(\n",
    "    valid_data, os.path.join(OUTPUT_DIR,IMAGE_DIR), os.path.join(OUTPUT_DIR,LABEL_DIR),\n",
    "    transform = transform,\n",
    "    resize = IMAGE_SIZE\n",
    ")\n",
    "test_dataset = SyntheticDataset(\n",
    "    test_data, os.path.join(OUTPUT_DIR,IMAGE_DIR), os.path.join(OUTPUT_DIR,LABEL_DIR),\n",
    "    transform = transform,\n",
    "    resize = IMAGE_SIZE,\n",
    "    test=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fixed_boxes)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fixed_boxes)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fixed_boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e72c3608-c213-4f07-89cc-3821bf70af2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T16:01:06.654106Z",
     "iopub.status.busy": "2025-08-07T16:01:06.653945Z",
     "iopub.status.idle": "2025-08-07T16:01:06.930282Z",
     "shell.execute_reply": "2025-08-07T16:01:06.929676Z",
     "shell.execute_reply.started": "2025-08-07T16:01:06.654092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test sizes: 3500/1250/250\n",
      "batch imgs: torch.Size([32, 3, 256, 256]) batch heatmaps: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train/Val/Test sizes: {len(train_dataset)}/{len(valid_dataset)}/{len(test_dataset)}\")\n",
    "for imgs, hms in train_loader:\n",
    "    print(\"batch imgs:\", imgs.shape, \"batch heatmaps:\", len(hms))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159f87a2-7933-4ea5-bc0a-3dbd8f4becbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T16:01:06.931479Z",
     "iopub.status.busy": "2025-08-07T16:01:06.931220Z",
     "iopub.status.idle": "2025-08-07T16:01:06.941238Z",
     "shell.execute_reply": "2025-08-07T16:01:06.940721Z",
     "shell.execute_reply.started": "2025-08-07T16:01:06.931458Z"
    }
   },
   "outputs": [],
   "source": [
    "class GridAnchorDetector(nn.Module):\n",
    "    def __init__(self, in_channels=64, num_anchors=3):\n",
    "        super().__init__()\n",
    "        self.box_head = nn.Conv2d(in_channels, num_anchors * 4, kernel_size=1)\n",
    "        self.cls_head = nn.Conv2d(in_channels, num_anchors, kernel_size=1)\n",
    "\n",
    "    def forward(self, feat):\n",
    "        B, C, H, W = feat.shape\n",
    "        raw_offsets = self.box_head(feat).view(B, -1, 4)\n",
    "        logits = self.cls_head(feat).view(B, -1)\n",
    "        \n",
    "        return raw_offsets, logits\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,kernel_size=3, padding=1, activation=nn.LeakyReLU, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            activation(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class AnchorGenerator():\n",
    "    def __init__(self, stride=4, scales=[16, 32, 64]):\n",
    "        self.stride = stride\n",
    "        self.scales = tuple(scales)  # Must be hashable\n",
    "\n",
    "    @lru_cache(maxsize=1)\n",
    "    def generate(self, image_height, image_width):\n",
    "        H, W = image_height // self.stride, image_width // self.stride\n",
    "        anchors = []\n",
    "        for i in range(H):\n",
    "            for j in range(W):\n",
    "                cx = (j + 0.5) * self.stride\n",
    "                cy = (i + 0.5) * self.stride\n",
    "                for scale in self.scales:\n",
    "                    anchors.append([\n",
    "                        cx - scale / 2,\n",
    "                        cy - scale / 2,\n",
    "                        cx + scale / 2,\n",
    "                        cy + scale / 2\n",
    "                    ])\n",
    "        \n",
    "        return torch.tensor(anchors)\n",
    "\n",
    "\n",
    "class RoundObjectDetector(nn.Module):\n",
    "    def __init__(self, image_size, iou_threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.iou_threshold = iou_threshold\n",
    "\n",
    "        self.conv1 = ConvBlock(3,  16, dropout=0.05)\n",
    "        self.conv2 = ConvBlock(16, 32, dropout=0.10)\n",
    "        self.conv3 = ConvBlock(32, 64, dropout=0.20)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.stride = self._infer_stride(torch.zeros(1, 3, *image_size))\n",
    "        self.anchor_generator = AnchorGenerator(image_size, self.stride)\n",
    "        self.anchors = self.anchor_generator.generate()\n",
    "        self.num_anchors = self.anchors.size(0) // ((image_size[0] // self.stride)*(image_size[1] // self.stride))\n",
    "        self.detector = GridAnchorDetector(in_channels=64, num_anchors=self.num_anchors)\n",
    "\n",
    "    def _infer_stride(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.conv1(x)\n",
    "            x = self.pool(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.pool(x)\n",
    "            x = self.conv3(x)\n",
    "            feature_map_size = x.shape[2:]  # (H, W)\n",
    "\n",
    "        stride_h = self.image_size[0] // feature_map_size[0]\n",
    "        stride_w = self.image_size[1] // feature_map_size[1]\n",
    "\n",
    "        assert stride_h == stride_w, \"Non-square stride detected\"\n",
    "        return stride_h\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        offsets, logits = self.detector(x)\n",
    "        boxes = decode_offsets_to_boxes(offsets, self.anchors)\n",
    "        \n",
    "        return boxes, logits\n",
    "\n",
    "\n",
    "def decode_offsets_to_boxes(offsets, anchors):\n",
    "    anchors = anchors.to(offsets.device)\n",
    "    anchor_centers = (anchors[..., :2] + anchors[..., 2:]) / 2\n",
    "    anchor_sizes = anchors[..., 2:] - anchors[..., :2]\n",
    "\n",
    "    pred_centers = offsets[..., :2] * anchor_sizes + anchor_centers\n",
    "    pred_sizes = torch.exp(offsets[..., 2:]) * anchor_sizes\n",
    "\n",
    "    boxes = torch.cat([\n",
    "        pred_centers - pred_sizes / 2,\n",
    "        pred_centers + pred_sizes / 2\n",
    "    ], dim=-1)\n",
    "    \n",
    "    return boxes  # [B, N, 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f939b7-b493-4ba5-acc3-b3790418a4bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T16:01:08.453674Z",
     "iopub.status.busy": "2025-08-07T16:01:08.453214Z",
     "iopub.status.idle": "2025-08-07T16:01:08.462570Z",
     "shell.execute_reply": "2025-08-07T16:01:08.462113Z",
     "shell.execute_reply.started": "2025-08-07T16:01:08.453656Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def assign_targets_to_anchors(gt_boxes, anchors, iou_threshold=0.5):\n",
    "    B, M, _ = gt_boxes.shape\n",
    "    N = anchors.size(0)\n",
    "    target_boxes = torch.zeros((B, N, 4), dtype=torch.float32, device=gt_boxes.device)\n",
    "    object_mask = torch.zeros((B, N), dtype=torch.float32, device=gt_boxes.device)\n",
    "\n",
    "    for b in range(B):\n",
    "        valid_boxes = gt_boxes[b][gt_boxes[b].abs().sum(dim=-1) > 0]  # [M', 4]\n",
    "        if valid_boxes.numel() == 0:\n",
    "            continue\n",
    "        \n",
    "        ious = compute_iou(valid_boxes, anchors)  # [M', N]\n",
    "        max_ious, max_idxs = ious.max(dim=0)  # [N]\n",
    "        object_mask[b] = (max_ious > iou_threshold).float()\n",
    "        target_boxes[b] = valid_boxes[max_idxs]  # assign best-matching box to each anchor\n",
    "        \n",
    "    return target_boxes, object_mask  # [B, N, 4], [B, N]\n",
    "\n",
    "\n",
    "def compute_iou(boxes1, boxes2):\n",
    "    # boxes1: [M, 4], boxes2: [N, 4]\n",
    "    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "\n",
    "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [M, N, 2]\n",
    "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [M, N, 2]\n",
    "\n",
    "    wh = (rb - lt).clamp(min=0)  # [M, N, 2]\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]  # [M, N]\n",
    "\n",
    "    union = area1[:, None] + area2 - inter\n",
    "    iou = inter / min(area1, area2)  # [M, N]\n",
    "    \n",
    "    return iou\n",
    "\n",
    "\n",
    "def match_anchors_to_gt(gt_boxes, anchors, iou_threshold=0.5):\n",
    "    B, M, _ = gt_boxes.shape\n",
    "    N = anchors.size(0)\n",
    "    matched_mask = torch.zeros((B, N), dtype=torch.float32, device=gt_boxes.device)\n",
    "\n",
    "    for b in range(B):\n",
    "        valid_boxes = gt_boxes[b][gt_boxes[b].abs().sum(dim=-1) > 0]  # [M', 4]\n",
    "        if valid_boxes.numel() == 0:\n",
    "            continue\n",
    "            \n",
    "        ious = compute_iou(valid_boxes, anchors)  # [M', N]\n",
    "        matched_mask[b] = (ious.max(dim=0).values > iou_threshold).float()\n",
    "        \n",
    "    return matched_mask  # [B, N]\n",
    "\n",
    "\n",
    "def focal_loss(logits, targets, alpha=0.25, gamma=2.0):\n",
    "    prob = torch.sigmoid(logits)\n",
    "    ce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "    p_t = prob * targets + (1 - prob) * (1 - targets)\n",
    "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
    "    alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "    \n",
    "    return (loss * alpha_t).mean()\n",
    "\n",
    "\n",
    "def encode_boxes_to_offsets(target_boxes, anchors):\n",
    "    # target_boxes: [B, N, 4], anchors: [N, 4]\n",
    "    anchor_centers = (anchors[:, :2] + anchors[:, 2:]) / 2  # [N,2]\n",
    "    anchor_sizes =  anchors[:, 2:] - anchors[:, :2]      # [N,2]\n",
    "\n",
    "    # expand to batch size\n",
    "    B = target_boxes.size(0)\n",
    "    anchor_centers = anchor_centers.unsqueeze(0).expand(B, -1, -1)\n",
    "    anchor_sizes = anchor_sizes.unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "    target_centers = (target_boxes[..., :2] + target_boxes[..., 2:]) / 2\n",
    "    target_sizes =  target_boxes[..., 2:] - target_boxes[..., :2]\n",
    " \n",
    "    offsets = torch.zeros_like(target_boxes)\n",
    "    offsets[..., :2] = (target_centers - anchor_centers) / anchor_sizes\n",
    "    offsets[..., 2:] = torch.log((target_sizes + 1e-8) / anchor_sizes)\n",
    "\n",
    "    return offsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eb2304e-5eac-49bc-b7db-0999267f7686",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T16:01:14.168327Z",
     "iopub.status.busy": "2025-08-07T16:01:14.168037Z",
     "iopub.status.idle": "2025-08-07T16:01:17.762926Z",
     "shell.execute_reply": "2025-08-07T16:01:17.762469Z",
     "shell.execute_reply.started": "2025-08-07T16:01:14.168310Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/hm_dataset_min_max/images/img_00000.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/hm_dataset_min_max/images/img_00000.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load and resize\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mresize(IMAGE_SIZE)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Generate anchors\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\PIL\\Image.py:3513\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[0;32m   3512\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fp)\n\u001b[1;32m-> 3513\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3514\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3515\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/hm_dataset_min_max/images/img_00000.jpg'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Path to your image\n",
    "img_path = \"data/hm_dataset_min_max/images/img_00000.jpg\"\n",
    "\n",
    "# Load and resize\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "img = img.resize(IMAGE_SIZE)\n",
    "\n",
    "# Generate anchors\n",
    "anchor_generator = AnchorGenerator(IMAGE_SIZE, 10)\n",
    "anchors = anchor_generator.generate()\n",
    "\n",
    "# Set up plot\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.imshow(img)\n",
    "ax.axis(\"off\")\n",
    "\n",
    "# Choose a colormap for different scales\n",
    "colors = [\"lime\", \"cyan\", \"magenta\"]\n",
    "\n",
    "# Draw each anchor\n",
    "for idx, box in enumerate(anchors):\n",
    "    xmin, ymin, xmax, ymax = box.tolist()\n",
    "    \n",
    "    # Determine color by scale index\n",
    "    scale_idx = idx % len(colors)\n",
    "    rect = patches.Rectangle(\n",
    "        (xmin, ymin),\n",
    "        xmax - xmin,\n",
    "        ymax - ymin,\n",
    "        linewidth=1,\n",
    "        edgecolor=colors[scale_idx],\n",
    "        facecolor=\"none\",\n",
    "        alpha=0.4\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "650d7845-d936-4ef3-bc09-ea89e502db2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T15:13:54.937643Z",
     "iopub.status.busy": "2025-08-06T15:13:54.937346Z",
     "iopub.status.idle": "2025-08-06T15:13:54.943796Z",
     "shell.execute_reply": "2025-08-06T15:13:54.943266Z",
     "shell.execute_reply.started": "2025-08-06T15:13:54.937623Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, anchors, epoch, warmup_epochs, lambda_cls, lambda_reg, max_grad_norm):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    use_focal = epoch > warmup_epochs\n",
    "    for images, gt_boxes in dataloader:\n",
    "        images = images.to(DEVICE)           # [B, 3, H, W]\n",
    "        gt_boxes = gt_boxes['l_boxes'].to(DEVICE)         # [B, M, 4] padded with zeros\n",
    "\n",
    "        # forward\n",
    "        pred_offsets, logits = model(images)   # [B, N, 4], [B, N]\n",
    "\n",
    "        # assign anchors → targets\n",
    "        target_boxes, object_mask = assign_targets_to_anchors(gt_boxes, anchors, iou_threshold=model.iou_threshold)\n",
    "\n",
    "        # encode boxes\n",
    "        target_offsets = encode_boxes_to_offsets(target_boxes, anchors)\n",
    "\n",
    "        # classification loss\n",
    "        if use_focal:\n",
    "            cls_loss = focal_loss(logits, object_mask)\n",
    "            \n",
    "        else:\n",
    "            cls_loss = F.binary_cross_entropy_with_logits(logits, object_mask)\n",
    "\n",
    "        # regression loss (only positives)\n",
    "        pos_mask = object_mask.bool()           # [B, N]\n",
    "        if pos_mask.any():\n",
    "            reg_loss = F.smooth_l1_loss(\n",
    "                pred_offsets[pos_mask],\n",
    "                target_offsets[pos_mask],\n",
    "                reduction='mean'\n",
    "            )\n",
    "        else:\n",
    "            reg_loss = torch.tensor(0.0, device=DEVICE)\n",
    "\n",
    "        # total loss\n",
    "        loss = lambda_cls * cls_loss + lambda_reg * reg_loss\n",
    "\n",
    "        # backward & optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    \n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def validate_one_epoch(model, dataloader, anchors, lambda_cls, lambda_reg):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, gt_boxes in dataloader:\n",
    "            images = images.to(DEVICE)\n",
    "            gt_boxes = gt_boxes['l_boxes'].to(DEVICE)\n",
    "\n",
    "            pred_offsets, logits = model(images)\n",
    "            target_boxes, object_mask = assign_targets_to_anchors(gt_boxes, anchors, iou_threshold=model.iou_threshold)\n",
    "            target_offsets = encode_boxes_to_offsets(target_boxes, anchors)\n",
    "\n",
    "            cls_loss = F.binary_cross_entropy_with_logits(logits, object_mask)\n",
    "            pos_mask = object_mask.bool()\n",
    "            if pos_mask.any():\n",
    "                reg_loss = F.smooth_l1_loss(\n",
    "                    pred_offsets[pos_mask],\n",
    "                    target_offsets[pos_mask],\n",
    "                    reduction='mean'\n",
    "                )\n",
    "            else:\n",
    "                reg_loss = torch.tensor(0.0, device=DEVICE)\n",
    "\n",
    "            loss = lambda_cls * cls_loss + lambda_reg * reg_loss\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e5e121a-1a78-4b0b-8e36-7438d76c31db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T15:13:56.168678Z",
     "iopub.status.busy": "2025-08-06T15:13:56.168366Z",
     "iopub.status.idle": "2025-08-06T15:13:56.183919Z",
     "shell.execute_reply": "2025-08-06T15:13:56.183438Z",
     "shell.execute_reply.started": "2025-08-06T15:13:56.168659Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "anchors = generate_anchors(IMAGE_SIZE).to(DEVICE)\n",
    "NUM_EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.02\n",
    "warmup_epochs = 5\n",
    "lambda_cls = 0.01\n",
    "lambda_reg = 1.0\n",
    "max_grad_norm = 1.0\n",
    "patience = 10\n",
    "\n",
    "# --- Model, Optimizer ---\n",
    "model = RoundObjectDetector(image_size=IMAGE_SIZE, anchors=anchors).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a60ebb-2696-49eb-a4c5-0a8649d25be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Early Stopping Setup ---\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, anchors, epoch, warmup_epochs, lambda_cls, lambda_reg, max_grad_norm)\n",
    "    val_loss = validate_one_epoch(model, valid_loader, anchors, lambda_cls, lambda_reg)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} — Train Loss: {train_loss:.4f}  Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"saved_models/best_detector.pth\")\n",
    "        epochs_no_improve = 0\n",
    "        \n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"No improvement for {patience} epochs. Early stopping.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b10a3568-5f80-4571-bd3b-930478aecb8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T15:15:26.541045Z",
     "iopub.status.busy": "2025-08-06T15:15:26.540751Z",
     "iopub.status.idle": "2025-08-06T15:15:26.551641Z",
     "shell.execute_reply": "2025-08-06T15:15:26.551187Z",
     "shell.execute_reply.started": "2025-08-06T15:15:26.541025Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RoundObjectDetector(image_size=IMAGE_SIZE, anchors=anchors).to(DEVICE)\n",
    "\n",
    "PATH = \"saved_models/best_detector.pth\"\n",
    "model.load_state_dict(torch.load(PATH, map_location=DEVICE))\n",
    "\n",
    "model.eval() is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8848e8bc-a440-4a6b-ba73-e6f2fdfc057a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T15:08:35.028071Z",
     "iopub.status.busy": "2025-08-06T15:08:35.027770Z",
     "iopub.status.idle": "2025-08-06T15:08:35.390787Z",
     "shell.execute_reply": "2025-08-06T15:08:35.390092Z",
     "shell.execute_reply.started": "2025-08-06T15:08:35.028053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01782929\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAHFCAYAAADv8c1wAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUDFJREFUeJzt3X9clfX9//Hnid+inBQEPBN/LSQJ7QcuRFdaKmSira20aKRpaKNJFMyyVmorzB9ZbX4qa03LLNoyW6USVuYn5s9IVv7ox8oUJogpHpQQEN7fP/p4fT2idsFEwD3ut9u53Xau63Vd1+t6c+w89z7XdY7DGGMEAACA0zqvpRsAAABoCwhNAAAANhCaAAAAbCA0AQAA2EBoAgAAsIHQBAAAYAOhCQAAwAZCEwAAgA2EJgAAABsITcAZ8Omnn+q2225Tz5495e/vr/bt2+uyyy7TnDlzdODAgWY99pYtWzR48GA5nU45HA49+eST+vDDD+VwOPThhx/+6Pbjx49Xjx49mrXHs8EYo5ycHF1xxRUKDQ2Vv7+/unbtqsTERP35z39u6fYabciQIXI4HHI4HDrvvPPUoUMHXXDBBbrxxhv1+uuvq76+vsE2PXr00Pjx4xt1nHXr1mnGjBk6ePBgo7Y78VjHXnOvv/56o/ZzOt9//71mzJhx0tfx4sWL5XA49O23356x4wE/xrulGwDauueff15paWmKiorS7373O0VHR6u2tlYff/yxnn32Wa1fv17Lly9vtuNPmDBBlZWVysnJUceOHdWjRw+1a9dO69evV3R0dLMdt7WZNm2aZs+erdTUVP3ud79Thw4dtGvXLn3wwQf6+9//rttvv72lW2y0Xr16aenSpZKkyspK7dy5U2+++aZuvPFGXXHFFXr77bfldDqt+uXLlysoKKhRx1i3bp1mzpyp8ePH6/zzz7e9XVOO1Vjff/+9Zs6cKemHEHm8kSNHav369erSpUuz9gAcj9AE/AfWr1+v3/zmNxo+fLjefPNN+fn5WeuGDx+uzMxM5ebmNmsPW7duVWpqqkaMGOGxfMCAAc163NakqqpKTz75pG699VY999xzHuvGjx9/0lmZ5u4nICDgP95PQEBAg7/j7bffrkWLFmnChAmaNGmSXnvtNWvdpZde+h8f88ccO7ezcazT6dy5szp37tyiPeC/Dx/PAf+B7OxsORwOPffccx6B6RhfX1+NHj3ael5fX685c+bowgsvlJ+fn0JDQ3XrrbequLjYY7shQ4YoJiZGmzdv1hVXXKF27dqpV69eeuyxx6wAcOzjiaNHj+qZZ56xPsqRdMqP5xYvXqyoqCj5+fmpT58+eumll056XjU1NXrkkUesPjt37qzbbrtN+/bt86jr0aOHkpKSlJubq8suu0wBAQG68MIL9Ze//KXBPv/9739r0qRJioiIkK+vr1wul2644Qbt3bvXqqmoqFBWVpZ69uwpX19f/eQnP1FGRoYqKytP81f4YRamurr6lLMO553n+Z+66upqPfzww+rTp4/8/f0VHBysq666SuvWrbNqjhw5omnTpnn0cueddzb4GOvYGLzxxhu69NJL5e/vb82OlJaWavLkyeratat8fX3Vs2dPzZw5U0ePHj3t+fyY2267Tddee63+9re/adeuXR69HP+RWX19vR555BFFRUUpICBA559/vvr166ennnpKkjRjxgz97ne/kyT17NnTeg0de92c7txO9VHgkSNHdM899yg8PFwBAQEaPHiwtmzZ4lEzZMiQBjNHkudHxd9++60VimbOnGn1duyYp/p47i9/+Ysuvvhi+fv7q1OnTrr++uu1Y8eOBsdp3769/vWvf+naa69V+/btFRERoczMTFVXV59y3AEZAE1y9OhR065dOxMXF2d7m0mTJhlJ5re//a3Jzc01zz77rOncubOJiIgw+/bts+oGDx5sgoODTWRkpHn22WfN6tWrTVpampFkXnzxRWOMMWVlZWb9+vVGkrnhhhvM+vXrzfr1640xxqxZs8ZIMmvWrLH2uWjRIiPJXHfddebtt982L7/8srngggtMRESE6d69u1VXV1dnrrnmGhMYGGhmzpxpVq9ebf785z+bn/zkJyY6Otp8//33Vm337t1N165dTXR0tHnppZfMu+++a2688UYjyaxdu9aqKy4uNl26dDEhISFm/vz55r333jOvvfaamTBhgtmxY4cxxpjKykpzySWXeNQ89dRTxul0mquvvtrU19efdmwvuOAC06FDB/P444+bHTt2nLK+trbWXHXVVcbb29tkZWWZlStXmrfeesvcf//95tVXXzXGGFNfX28SExONt7e3efDBB01eXp6ZN2+eCQwMNJdeeqk5cuSIxxh06dLF9OrVy/zlL38xa9asMZs2bTIlJSXW2C5cuNC899575g9/+IPx8/Mz48ePP+25HHsNXHTRRadc/+yzzxpJZsmSJR69jBs3zno+a9Ys4+XlZaZPn27ef/99k5uba5588kkzY8YMY4wxRUVFZsqUKUaSeeONN6zXkNvtPu25nexYx15zERERDV5jQUFB5uuvv/Y4t8GDBzc4p3HjxlmvxSNHjpjc3FwjyUycONHq7V//+pcx5v+/nnfu3Gltn52dbSSZm2++2axYscK89NJLplevXsbpdJovv/zS4zi+vr6mT58+Zt68eea9994zDz30kHE4HGbmzJmn/8PgvxqhCWii0tJSI8ncdNNNtup37NhhJJm0tDSP5Rs3bjSSzP33328tGzx4sJFkNm7c6FEbHR1tEhMTPZZJMnfeeafHshNDU11dnXG5XOayyy7zCBPffvut8fHx8QhNr776qpFkli1b5rHPzZs3G0nm6aeftpZ1797d+Pv7m127dlnLqqqqTKdOnczkyZOtZRMmTDA+Pj5m+/btpxyfWbNmmfPOO89s3rzZY/nrr79uJJmVK1eecltjjNm0aZPp1q2bkWQkmQ4dOpikpCTz0ksveZzzSy+9ZCSZ559//pT7OvZmPWfOHI/lr732mpFknnvuOY8x8PLyMl988YVH7eTJk0379u09xsYYY+bNm2ckmW3btp32fH4sNK1atcpIMrNnz/bo5fggk5SUZC655JLTHmfu3LkNwsfx+zvZuZ3sWMdec6d6jd1+++0e5/ZjockYY/bt22ckmenTpzeoPTE0lZeXm4CAAHPttdd61O3evdv4+fmZ5ORkj+NIMn/96189aq+99loTFRXV4FjAMXw8B5wla9askaQGH2lcfvnl6tOnj95//32P5eHh4br88ss9lvXr18/j4xi7vvjiC+3Zs0fJycnWR3iS1L17dw0cONCj9p133tH555+vUaNG6ejRo9bjkksuUXh4eIOP/C655BJ169bNeu7v76/evXt79Llq1SpdddVV6tOnzyl7fOeddxQTE6NLLrnE47iJiYm27gT82c9+pn/961/Kzc3V/fffr/j4eL3//vu69dZbNXr0aBljrF78/f01YcKEU+7rgw8+kNTwb3XjjTcqMDCwwd+qX79+6t27d4Pzueqqq+RyuTzO59i1Z2vXrj3t+fyYY+dzOpdffrn++c9/Ki0tTe+++64qKioafZyTndvpnOo1duz131zWr1+vqqqqBn+ziIgIXX311Q3+Zg6HQ6NGjfJY1tR/X/jvQWgCmigkJETt2rXTzp07bdXv379fkk563Y3L5bLWHxMcHNygzs/PT1VVVY3u9di+w8PDG6w7cdnevXt18OBB+fr6ysfHx+NRWlqq7777rtF97tu3T127dj1tj3v37tWnn37a4JgdOnSQMabBcU/Gx8dHiYmJevTRR/Xuu++qqKhIQ4YM0TvvvKNVq1ZZvbhcrgbXOR1v//798vb2bnChscPhUHh4eIO/1cn+pnv37tXbb7/d4HwuuugiSbJ1Pqdz7M3d5XKdsmbatGmaN2+eNmzYoBEjRig4OFhDhw7Vxx9/bPs4jb077VSvsRPH7Exr7L+vdu3ayd/f32OZn5+fjhw50nxNos3j7jmgiby8vDR06FCtWrVKxcXFPxoKjoWLkpKSBrV79uxRSEhIs/V67NilpaUN1p24LCQkRMHBwae8669Dhw6NPn7nzp0bXOx+opCQEAUEBJz0IvJj6xsrODhYGRkZ+vDDD7V161Zde+216ty5s/Lz81VfX3/K4BQcHKyjR49q3759HsHJGKPS0lL97Gc/86g/fmbl+H779eunRx999KTHOF3YseOtt96Sw+HQlVdeecoab29v3XPPPbrnnnt08OBBvffee7r//vuVmJiooqIitWvX7kePc7JzO51TvcaOD9f+/v5yu90N6v6TIHn8v68TNfe/L/z3YKYJ+A9MmzZNxhilpqaqpqamwfra2lq9/fbbkqSrr75akvTyyy971GzevFk7duzQ0KFDm63PqKgodenSRa+++qrHxzq7du3yuGNMkpKSkrR//37V1dWpf//+DR5RUVGNPv6IESO0Zs0affHFF6esSUpK0tdff63g4OCTHvd0X8BZW1t7ypmMY3dOHQspI0aM0JEjR7R48eJT7u/Y3+LEv9WyZctUWVlp62+VlJSkrVu36qc//elJz+c/CU2LFi3SqlWrdPPNN3t8NHo6559/vm644QbdeeedOnDggHXX2bG7Ppsyg3kyp3qNHX+3XI8ePfTll1963Km2f//+Bq/FxvQWHx+vgICABn+z4uJiffDBB8367wv/PZhpAv4D8fHxeuaZZ5SWlqbY2Fj95je/0UUXXaTa2lpt2bJFzz33nGJiYjRq1ChFRUVp0qRJ+tOf/qTzzjtPI0aM0LfffqsHH3xQERERuvvuu5utz/POO09/+MMfdPvtt+v6669XamqqDh48qBkzZjT4OOWmm27S0qVLde211+quu+7S5ZdfLh8fHxUXF2vNmjW67rrrdP311zfq+A8//LBWrVqlK6+8Uvfff7/69u2rgwcPKjc3V/fcc48uvPBCZWRkaNmyZbryyit19913q1+/fqqvr9fu3buVl5enzMxMxcXFnXT/brdbPXr00I033qhhw4YpIiJChw8f1ocffqinnnpKffr00S9/+UtJ0s0336xFixbpjjvu0BdffKGrrrpK9fX12rhxo/r06aObbrpJw4cPV2Jiou69915VVFRo0KBB+vTTTzV9+nRdeumlSklJsXXOq1ev1sCBA5Wenq6oqCgdOXJE3377rVauXKlnn332R2cnq6qqtGHDBut/f/PNN3rzzTf1zjvvaPDgwXr22WdPu/2oUaMUExOj/v37q3Pnztq1a5eefPJJde/eXZGRkZKkvn37SpKeeuopjRs3Tj4+PoqKimrSjKIklZWVWa8xt9ut6dOny9/fX9OmTbNqUlJStHDhQv36179Wamqq9u/frzlz5jT4sswOHTqoe/fu+vvf/66hQ4eqU6dOCgkJOWmAPv/88/Xggw/q/vvv16233qqbb75Z+/fv18yZM+Xv76/p06c36XwADy14ETpwzigsLDTjxo0z3bp1M76+vtat6Q899JApKyuz6urq6szs2bNN7969jY+PjwkJCTG//vWvTVFRkcf+TnXn1Il3Fxlj7+65Y/785z+byMhI4+vra3r37m3+8pe/nHSftbW1Zt68eebiiy82/v7+pn379ubCCy80kydPNl999ZVV1717dzNy5MgGfZ7s7qiioiIzYcIEEx4ebnx8fIzL5TJjxowxe/futWoOHz5sfv/735uoqCjj6+trnE6n6du3r7n77rtNaWlpg+McU11dbebNm2dGjBhhunXrZvz8/Iy/v7/p06ePmTp1qtm/f79HfVVVlXnooYessQgODjZXX321WbdunUfNvffea7p37258fHxMly5dzG9+8xtTXl7usa9TjYExP9z9lZ6ebnr27Gl8fHxMp06dTGxsrHnggQfM4cOHT3k+x8ZQ/3cnoCQTGBhoevXqZW644Qbzt7/9zdTV1TXY5sQ72h5//HEzcOBAExISYnx9fU23bt3MxIkTzbfffuux3bRp04zL5TLnnXeex+vmdOd2qrvnlixZYtLT003nzp2Nn5+fueKKK8zHH3/cYPsXX3zR9OnTx/j7+5vo6Gjz2muvnfS1+N5775lLL73U+Pn5GUnWMU/2lQPG/PAa79evn/X6ue666xrcqThu3DgTGBjYoKfp06cb3hZxOg5jbNyCAQAA8F+Oa5oAAABsIDQBAADYQGgCAACwgdAEAABgA6EJAADABkITAACADXy55RlUX1+vPXv2qEOHDo3+6QEAANAyjDE6dOjQj/4uJaHpDNqzZ48iIiJaug0AANAERUVFp/2mfkLTGXTsZweKiooa/BwAAABonSoqKhQREfGjPx9EaDqDjn0kFxQURGgCAKCN+bFLa7gQHAAAwAZCEwAAgA2EJgAAABsITQAAADYQmgAAAGwgNAEAANhAaAIAALCB0AQAAGADoQkAAMAGQhMAAIANhCYAAAAbCE0AAAA2EJoAAABsIDQBAADYQGgCAACwwbulG4A9Pe5b0Wz7/vaxkc22bwAAzhXMNAEAANhAaAIAALCB0AQAAGBDi4amo0eP6ve//7169uypgIAA9erVSw8//LDq6+utGmOMZsyYIZfLpYCAAA0ZMkTbtm3z2E91dbWmTJmikJAQBQYGavTo0SouLvaoKS8vV0pKipxOp5xOp1JSUnTw4EGPmt27d2vUqFEKDAxUSEiI0tPTVVNT02znDwAA2o4WDU2zZ8/Ws88+qwULFmjHjh2aM2eO5s6dqz/96U9WzZw5czR//nwtWLBAmzdvVnh4uIYPH65Dhw5ZNRkZGVq+fLlycnKUn5+vw4cPKykpSXV1dVZNcnKyCgsLlZubq9zcXBUWFiolJcVaX1dXp5EjR6qyslL5+fnKycnRsmXLlJmZeXYGAwAAtGoOY4xpqYMnJSUpLCxML7zwgrXsV7/6ldq1a6clS5bIGCOXy6WMjAzde++9kn6YVQoLC9Ps2bM1efJkud1ude7cWUuWLNHYsWMlSXv27FFERIRWrlypxMRE7dixQ9HR0dqwYYPi4uIkSRs2bFB8fLw+//xzRUVFadWqVUpKSlJRUZFcLpckKScnR+PHj1dZWZmCgoJ+9HwqKirkdDrldrtt1TcGd88BANA87L5/t+hM089//nO9//77+vLLLyVJ//znP5Wfn69rr71WkrRz506VlpYqISHB2sbPz0+DBw/WunXrJEkFBQWqra31qHG5XIqJibFq1q9fL6fTaQUmSRowYICcTqdHTUxMjBWYJCkxMVHV1dUqKCg4af/V1dWqqKjweAAAgHNTi35P07333iu3260LL7xQXl5eqqur06OPPqqbb75ZklRaWipJCgsL89guLCxMu3btsmp8fX3VsWPHBjXHti8tLVVoaGiD44eGhnrUnHicjh07ytfX16o50axZszRz5szGnjYAAGiDWnSm6bXXXtPLL7+sV155RZ988olefPFFzZs3Ty+++KJHncPh8HhujGmw7EQn1pysvik1x5s2bZrcbrf1KCoqOm1PAACg7WrRmabf/e53uu+++3TTTTdJkvr27atdu3Zp1qxZGjdunMLDwyX9MAvUpUsXa7uysjJrVig8PFw1NTUqLy/3mG0qKyvTwIEDrZq9e/c2OP6+ffs89rNx40aP9eXl5aqtrW0wA3WMn5+f/Pz8mnr6AACgDWnRmabvv/9e553n2YKXl5f1lQM9e/ZUeHi4Vq9eba2vqanR2rVrrUAUGxsrHx8fj5qSkhJt3brVqomPj5fb7damTZusmo0bN8rtdnvUbN26VSUlJVZNXl6e/Pz8FBsbe4bPHAAAtDUtOtM0atQoPfroo+rWrZsuuugibdmyRfPnz9eECRMk/fBxWUZGhrKzsxUZGanIyEhlZ2erXbt2Sk5OliQ5nU5NnDhRmZmZCg4OVqdOnZSVlaW+fftq2LBhkqQ+ffrommuuUWpqqhYuXChJmjRpkpKSkhQVFSVJSkhIUHR0tFJSUjR37lwdOHBAWVlZSk1NPeN3wgEAgLanRUPTn/70Jz344INKS0tTWVmZXC6XJk+erIceesiqmTp1qqqqqpSWlqby8nLFxcUpLy9PHTp0sGqeeOIJeXt7a8yYMaqqqtLQoUO1ePFieXl5WTVLly5Venq6dZfd6NGjtWDBAmu9l5eXVqxYobS0NA0aNEgBAQFKTk7WvHnzzsJIAACA1q5Fv6fpXMP3NAEA0Pa0ie9pAgAAaCsITQAAADYQmgAAAGwgNAEAANhAaAIAALCB0AQAAGADoQkAAMAGQhMAAIANhCYAAAAbCE0AAAA2EJoAAABsIDQBAADYQGgCAACwgdAEAABgA6EJAADABkITAACADYQmAAAAGwhNAAAANhCaAAAAbCA0AQAA2EBoAgAAsIHQBAAAYAOhCQAAwAZCEwAAgA2EJgAAABsITQAAADYQmgAAAGwgNAEAANhAaAIAALCB0AQAAGADoQkAAMAGQhMAAIANLRqaevToIYfD0eBx5513SpKMMZoxY4ZcLpcCAgI0ZMgQbdu2zWMf1dXVmjJlikJCQhQYGKjRo0eruLjYo6a8vFwpKSlyOp1yOp1KSUnRwYMHPWp2796tUaNGKTAwUCEhIUpPT1dNTU2znj8AAGg7WjQ0bd68WSUlJdZj9erVkqQbb7xRkjRnzhzNnz9fCxYs0ObNmxUeHq7hw4fr0KFD1j4yMjK0fPly5eTkKD8/X4cPH1ZSUpLq6uqsmuTkZBUWFio3N1e5ubkqLCxUSkqKtb6urk4jR45UZWWl8vPzlZOTo2XLlikzM/MsjQQAAGjtHMYY09JNHJORkaF33nlHX331lSTJ5XIpIyND9957r6QfZpXCwsI0e/ZsTZ48WW63W507d9aSJUs0duxYSdKePXsUERGhlStXKjExUTt27FB0dLQ2bNiguLg4SdKGDRsUHx+vzz//XFFRUVq1apWSkpJUVFQkl8slScrJydH48eNVVlamoKAgW/1XVFTI6XTK7Xbb3sauHvetOKP7O963j41stn0DANDa2X3/bjXXNNXU1Ojll1/WhAkT5HA4tHPnTpWWliohIcGq8fPz0+DBg7Vu3TpJUkFBgWpraz1qXC6XYmJirJr169fL6XRagUmSBgwYIKfT6VETExNjBSZJSkxMVHV1tQoKCk7Zc3V1tSoqKjweAADg3NRqQtObb76pgwcPavz48ZKk0tJSSVJYWJhHXVhYmLWutLRUvr6+6tix42lrQkNDGxwvNDTUo+bE43Ts2FG+vr5WzcnMmjXLuk7K6XQqIiKiEWcMAADaklYTml544QWNGDHCY7ZHkhwOh8dzY0yDZSc6seZk9U2pOdG0adPkdrutR1FR0Wn7AgAAbVerCE27du3Se++9p9tvv91aFh4eLkkNZnrKysqsWaHw8HDV1NSovLz8tDV79+5tcMx9+/Z51Jx4nPLyctXW1jaYgTqen5+fgoKCPB4AAODc1CpC06JFixQaGqqRI///Bck9e/ZUeHi4dUed9MN1T2vXrtXAgQMlSbGxsfLx8fGoKSkp0datW62a+Ph4ud1ubdq0yarZuHGj3G63R83WrVtVUlJi1eTl5cnPz0+xsbHNc9IAAKBN8W7pBurr67Vo0SKNGzdO3t7/vx2Hw6GMjAxlZ2crMjJSkZGRys7OVrt27ZScnCxJcjqdmjhxojIzMxUcHKxOnTopKytLffv21bBhwyRJffr00TXXXKPU1FQtXLhQkjRp0iQlJSUpKipKkpSQkKDo6GilpKRo7ty5OnDggLKyspSamsrsEQAAkNQKQtN7772n3bt3a8KECQ3WTZ06VVVVVUpLS1N5ebni4uKUl5enDh06WDVPPPGEvL29NWbMGFVVVWno0KFavHixvLy8rJqlS5cqPT3dustu9OjRWrBggbXey8tLK1asUFpamgYNGqSAgAAlJydr3rx5zXjmAACgLWlV39PU1vE9TQAAtD1t7nuaAAAAWjNCEwAAgA2EJgAAABsITQAAADYQmgAAAGwgNAEAANhAaAIAALCB0AQAAGADoQkAAMAGQhMAAIANhCYAAAAbCE0AAAA2EJoAAABsIDQBAADYQGgCAACwgdAEAABgA6EJAADABkITAACADYQmAAAAGwhNAAAANhCaAAAAbCA0AQAA2EBoAgAAsIHQBAAAYAOhCQAAwAZCEwAAgA2EJgAAABsITQAAADYQmgAAAGwgNAEAANhAaAIAALCB0AQAAGBDi4emf//73/r1r3+t4OBgtWvXTpdccokKCgqs9cYYzZgxQy6XSwEBARoyZIi2bdvmsY/q6mpNmTJFISEhCgwM1OjRo1VcXOxRU15erpSUFDmdTjmdTqWkpOjgwYMeNbt379aoUaMUGBiokJAQpaenq6amptnOHQAAtB0tGprKy8s1aNAg+fj4aNWqVdq+fbsef/xxnX/++VbNnDlzNH/+fC1YsECbN29WeHi4hg8frkOHDlk1GRkZWr58uXJycpSfn6/Dhw8rKSlJdXV1Vk1ycrIKCwuVm5ur3NxcFRYWKiUlxVpfV1enkSNHqrKyUvn5+crJydGyZcuUmZl5VsYCAAC0bg5jjGmpg9933336xz/+oY8++uik640xcrlcysjI0L333ivph1mlsLAwzZ49W5MnT5bb7Vbnzp21ZMkSjR07VpK0Z88eRUREaOXKlUpMTNSOHTsUHR2tDRs2KC4uTpK0YcMGxcfH6/PPP1dUVJRWrVqlpKQkFRUVyeVySZJycnI0fvx4lZWVKSgo6EfPp6KiQk6nU26321Z9Y/S4b8UZ3d/xvn1sZLPtGwCA1s7u+3eLzjS99dZb6t+/v2688UaFhobq0ksv1fPPP2+t37lzp0pLS5WQkGAt8/Pz0+DBg7Vu3TpJUkFBgWpraz1qXC6XYmJirJr169fL6XRagUmSBgwYIKfT6VETExNjBSZJSkxMVHV1tcfHhcerrq5WRUWFxwMAAJybWjQ0ffPNN3rmmWcUGRmpd999V3fccYfS09P10ksvSZJKS0slSWFhYR7bhYWFWetKS0vl6+urjh07nrYmNDS0wfFDQ0M9ak48TseOHeXr62vVnGjWrFnWNVJOp1MRERGNHQIAANBGtGhoqq+v12WXXabs7Gxdeumlmjx5slJTU/XMM8941DkcDo/nxpgGy050Ys3J6ptSc7xp06bJ7XZbj6KiotP2BAAA2q4WDU1dunRRdHS0x7I+ffpo9+7dkqTw8HBJajDTU1ZWZs0KhYeHq6amRuXl5aet2bt3b4Pj79u3z6PmxOOUl5ertra2wQzUMX5+fgoKCvJ4AACAc1OLhqZBgwbpiy++8Fj25Zdfqnv37pKknj17Kjw8XKtXr7bW19TUaO3atRo4cKAkKTY2Vj4+Ph41JSUl2rp1q1UTHx8vt9utTZs2WTUbN26U2+32qNm6datKSkqsmry8PPn5+Sk2NvYMnzkAAGhrvFvy4HfffbcGDhyo7OxsjRkzRps2bdJzzz2n5557TtIPH5dlZGQoOztbkZGRioyMVHZ2ttq1a6fk5GRJktPp1MSJE5WZmang4GB16tRJWVlZ6tu3r4YNGybph9mra665RqmpqVq4cKEkadKkSUpKSlJUVJQkKSEhQdHR0UpJSdHcuXN14MABZWVlKTU1lRkkAADQsqHpZz/7mZYvX65p06bp4YcfVs+ePfXkk0/qlltusWqmTp2qqqoqpaWlqby8XHFxccrLy1OHDh2smieeeELe3t4aM2aMqqqqNHToUC1evFheXl5WzdKlS5Wenm7dZTd69GgtWLDAWu/l5aUVK1YoLS1NgwYNUkBAgJKTkzVv3ryzMBIAAKC1a9HvaTrX8D1NAAC0PW3ie5oAAADaCkITAACADYQmAAAAGwhNAAAANhCaAAAAbCA0AQAA2EBoAgAAsIHQBAAAYAOhCQAAwAZCEwAAgA2EJgAAABsITQAAADYQmgAAAGwgNAEAANhAaAIAALCB0AQAAGADoQkAAMAGQhMAAIANhCYAAAAbCE0AAAA2EJoAAABsIDQBAADYQGgCAACwgdAEAABgA6EJAADABkITAACADYQmAAAAGwhNAAAANhCaAAAAbCA0AQAA2EBoAgAAsIHQBAAAYEOLhqYZM2bI4XB4PMLDw631xhjNmDFDLpdLAQEBGjJkiLZt2+axj+rqak2ZMkUhISEKDAzU6NGjVVxc7FFTXl6ulJQUOZ1OOZ1OpaSk6ODBgx41u3fv1qhRoxQYGKiQkBClp6erpqam2c4dAAC0LU0KTTt37jxjDVx00UUqKSmxHp999pm1bs6cOZo/f74WLFigzZs3Kzw8XMOHD9ehQ4esmoyMDC1fvlw5OTnKz8/X4cOHlZSUpLq6OqsmOTlZhYWFys3NVW5urgoLC5WSkmKtr6ur08iRI1VZWan8/Hzl5ORo2bJlyszMPGPnCQAA2jbvpmx0wQUX6Morr9TEiRN1ww03yN/fv+kNeHt7zC4dY4zRk08+qQceeEC//OUvJUkvvviiwsLC9Morr2jy5Mlyu9164YUXtGTJEg0bNkyS9PLLLysiIkLvvfeeEhMTtWPHDuXm5mrDhg2Ki4uTJD3//POKj4/XF198oaioKOXl5Wn79u0qKiqSy+WSJD3++OMaP368Hn30UQUFBTX5/AAAwLmhSTNN//znP3XppZcqMzNT4eHhmjx5sjZt2tSkBr766iu5XC717NlTN910k7755htJP8xmlZaWKiEhwar18/PT4MGDtW7dOklSQUGBamtrPWpcLpdiYmKsmvXr18vpdFqBSZIGDBggp9PpURMTE2MFJklKTExUdXW1CgoKTtl7dXW1KioqPB4AAODc1KTQFBMTo/nz5+vf//63Fi1apNLSUv385z/XRRddpPnz52vfvn229hMXF6eXXnpJ7777rp5//nmVlpZq4MCB2r9/v0pLSyVJYWFhHtuEhYVZ60pLS+Xr66uOHTuetiY0NLTBsUNDQz1qTjxOx44d5evra9WczKxZs6zrpJxOpyIiImydNwAAaHv+owvBvb29df311+uvf/2rZs+era+//lpZWVnq2rWrbr31VpWUlJx2+xEjRuhXv/qV+vbtq2HDhmnFihWSfvgY7hiHw+GxjTGmwbITnVhzsvqm1Jxo2rRpcrvd1qOoqOi0fQEAgLbrPwpNH3/8sdLS0tSlSxfNnz9fWVlZ+vrrr/XBBx/o3//+t6677rpG7S8wMFB9+/bVV199ZV3ndOJMT1lZmTUrFB4erpqaGpWXl5+2Zu/evQ2OtW/fPo+aE49TXl6u2traBjNQx/Pz81NQUJDHAwAAnJuaFJrmz5+vvn37auDAgdqzZ49eeukl7dq1S4888oh69uypQYMGaeHChfrkk08atd/q6mrt2LFDXbp0Uc+ePRUeHq7Vq1db62tqarR27VoNHDhQkhQbGysfHx+PmpKSEm3dutWqiY+Pl9vt9rjmauPGjXK73R41W7du9ZgZy8vLk5+fn2JjYxs/QAAA4JzTpLvnnnnmGU2YMEG33XbbSe98k6Ru3brphRdeOO1+srKyNGrUKHXr1k1lZWV65JFHVFFRoXHjxsnhcCgjI0PZ2dmKjIxUZGSksrOz1a5dOyUnJ0uSnE6nJk6cqMzMTAUHB6tTp07KysqyPu6TpD59+uiaa65RamqqFi5cKEmaNGmSkpKSFBUVJUlKSEhQdHS0UlJSNHfuXB04cEBZWVlKTU1l9ggAAEhqYmj66quvfrTG19dX48aNO21NcXGxbr75Zn333Xfq3LmzBgwYoA0bNqh79+6SpKlTp6qqqkppaWkqLy9XXFyc8vLy1KFDB2sfTzzxhLy9vTVmzBhVVVVp6NChWrx4sby8vKyapUuXKj093brLbvTo0VqwYIG13svLSytWrFBaWpoGDRqkgIAAJScna968eY0aFwAAcO5yGGNMYzdatGiR2rdvrxtvvNFj+d/+9jd9//33PxqWzlUVFRVyOp1yu91nfIaqx30rzuj+jvftYyObbd8AALR2dt+/m3RN02OPPaaQkJAGy0NDQ5Wdnd2UXQIAALRqTQpNu3btUs+ePRss7969u3bv3v0fNwUAANDaNCk0hYaG6tNPP22w/J///KeCg4P/46YAAABamyaFpptuuknp6elas2aN6urqVFdXpw8++EB33XWXbrrppjPdIwAAQItr0t1zjzzyiHbt2qWhQ4fK2/uHXdTX1+vWW2/lmiYAAHBOalJo8vX11WuvvaY//OEP+uc//6mAgAD17dvX+qoAAACAc02TQtMxvXv3Vu/evc9ULwAAAK1Wk0JTXV2dFi9erPfff19lZWWqr6/3WP/BBx+ckeYAAABaiyaFprvuukuLFy/WyJEjFRMTI4fDcab7AgAAaFWaFJpycnL017/+Vddee+2Z7gcAAKBVatJXDvj6+uqCCy44070AAAC0Wk0KTZmZmXrqqafUhJ+tAwAAaJOa9PFcfn6+1qxZo1WrVumiiy6Sj4+Px/o33njjjDQHAADQWjQpNJ1//vm6/vrrz3QvAAAArVaTQtOiRYvOdB8AAACtWpOuaZKko0eP6r333tPChQt16NAhSdKePXt0+PDhM9YcAABAa9GkmaZdu3bpmmuu0e7du1VdXa3hw4erQ4cOmjNnjo4cOaJnn332TPcJAADQopo003TXXXepf//+Ki8vV0BAgLX8+uuv1/vvv3/GmgMAAGgtmnz33D/+8Q/5+vp6LO/evbv+/e9/n5HGAAAAWpMmzTTV19errq6uwfLi4mJ16NDhP24KAACgtWlSaBo+fLiefPJJ67nD4dDhw4c1ffp0floFAACck5r08dwTTzyhq666StHR0Tpy5IiSk5P11VdfKSQkRK+++uqZ7hEAAKDFNSk0uVwuFRYW6tVXX9Unn3yi+vp6TZw4UbfccovHheEAAADniiaFJkkKCAjQhAkTNGHChDPZDwAAQKvUpND00ksvnXb9rbfe2qRmAAAAWqsmhaa77rrL43ltba2+//57+fr6ql27doQmAABwzmnS3XPl5eUej8OHD+uLL77Qz3/+cy4EBwAA56Qm//bciSIjI/XYY481mIUCAAA4F5yx0CRJXl5e2rNnz5ncJQAAQKvQpGua3nrrLY/nxhiVlJRowYIFGjRo0BlpDAAAoDVpUmj6xS9+4fHc4XCoc+fOuvrqq/X444+fib4AAABalSaFpvr6+jPdBwAAQKt2Rq9p+k/MmjVLDodDGRkZ1jJjjGbMmCGXy6WAgAANGTJE27Zt89iuurpaU6ZMUUhIiAIDAzV69GgVFxd71JSXlyslJUVOp1NOp1MpKSk6ePCgR83u3bs1atQoBQYGKiQkROnp6aqpqWmu0wUAAG1Mk2aa7rnnHtu18+fP/9GazZs367nnnlO/fv08ls+ZM0fz58/X4sWL1bt3bz3yyCMaPny4vvjiC3Xo0EGSlJGRobfffls5OTkKDg5WZmamkpKSVFBQIC8vL0lScnKyiouLlZubK0maNGmSUlJS9Pbbb0uS6urqNHLkSHXu3Fn5+fnav3+/xo0bJ2OM/vSnP9k+VwAAcO5qUmjasmWLPvnkEx09elRRUVGSpC+//FJeXl667LLLrDqHw/Gj+zp8+LBuueUWPf/883rkkUes5cYYPfnkk3rggQf0y1/+UpL04osvKiwsTK+88oomT54st9utF154QUuWLNGwYcMkSS+//LIiIiL03nvvKTExUTt27FBubq42bNiguLg4SdLzzz+v+Ph4ffHFF4qKilJeXp62b9+uoqIiuVwuSdLjjz+u8ePH69FHH1VQUFBThgkAAJxDmvTx3KhRozR48GAVFxfrk08+0SeffKKioiJdddVVSkpK0po1a7RmzRp98MEHP7qvO++8UyNHjrRCzzE7d+5UaWmpEhISrGV+fn4aPHiw1q1bJ0kqKChQbW2tR43L5VJMTIxVs379ejmdTiswSdKAAQPkdDo9amJiYqzAJEmJiYmqrq5WQUHBKXuvrq5WRUWFxwMAAJybmhSaHn/8cc2aNUsdO3a0lnXs2FGPPPJIo+6ey8nJ0SeffKJZs2Y1WFdaWipJCgsL81geFhZmrSstLZWvr69HHyerCQ0NbbD/0NBQj5oTj9OxY0f5+vpaNScza9Ys6zopp9OpiIiIHztlAADQRjUpNFVUVGjv3r0NlpeVlenQoUO29lFUVKS77rpLL7/8svz9/U9Zd+JHfMaYH/3Y78Sak9U3peZE06ZNk9vtth5FRUWn7QsAALRdTQpN119/vW677Ta9/vrrKi4uVnFxsV5//XVNnDjRuv7oxxQUFKisrEyxsbHy9vaWt7e31q5dqz/+8Y/y9va2Zn5OnOkpKyuz1oWHh6umpkbl5eWnrTlZwNu3b59HzYnHKS8vV21tbYMZqOP5+fkpKCjI4wEAAM5NTQpNzz77rEaOHKlf//rX6t69u7p3765bbrlFI0aM0NNPP21rH0OHDtVnn32mwsJC69G/f3/dcsstKiwsVK9evRQeHq7Vq1db29TU1Gjt2rUaOHCgJCk2NlY+Pj4eNSUlJdq6datVEx8fL7fbrU2bNlk1GzdulNvt9qjZunWrSkpKrJq8vDz5+fkpNja2KUMEAADOMU26e65du3Z6+umnNXfuXH399dcyxuiCCy5QYGCg7X106NBBMTExHssCAwMVHBxsLc/IyFB2drYiIyMVGRmp7OxstWvXTsnJyZIkp9OpiRMnKjMzU8HBwerUqZOysrLUt29f68LyPn366JprrlFqaqoWLlwo6YevHEhKSrLu/EtISFB0dLRSUlI0d+5cHThwQFlZWUpNTWX2CAAASGpiaDqmpKREJSUluvLKKxUQEGDreqPGmDp1qqqqqpSWlqby8nLFxcUpLy/P+o4mSXriiSfk7e2tMWPGqKqqSkOHDtXixYut72iSpKVLlyo9Pd26y2706NFasGCBtd7Ly0srVqxQWlqaBg0apICAACUnJ2vevHln7FwAAEDb5jDGmMZutH//fo0ZM0Zr1qyRw+HQV199pV69emnixIk6//zz/2t/f66iokJOp1Nut/uMz1D1uG/FGd3f8b59bGSz7RsAgNbO7vt3k65puvvuu+Xj46Pdu3erXbt21vKxY8da37oNAABwLmnSx3N5eXl699131bVrV4/lkZGR2rVr1xlpDAAAoDVp0kxTZWWlxwzTMd999538/Pz+46YAAABamyaFpiuvvFIvvfSS9dzhcKi+vl5z587VVVdddcaaAwAAaC2a9PHc3LlzNWTIEH388ceqqanR1KlTtW3bNh04cED/+Mc/znSPAAAALa5JM03R0dH69NNPdfnll2v48OGqrKzUL3/5S23ZskU//elPz3SPAAAALa7RM021tbVKSEjQwoULNXPmzOboCQAAoNVp9EyTj4+Ptm7deka/xBIAAKC1a9LHc7feeqteeOGFM90LAABAq9WkC8Framr05z//WatXr1b//v0b/Obc/Pnzz0hzAAAArUWjQtM333yjHj16aOvWrbrsssskSV9++aVHDR/bAQCAc1GjQlNkZKRKSkq0Zs0aST/8bMof//hHhYWFNUtzAAAArUWjrmk68bd9V61apcrKyjPaEAAAQGvUpAvBjzkxRAEAAJyrGhWaHA5Hg2uWuIYJAAD8N2jUNU3GGI0fP976Ud4jR47ojjvuaHD33BtvvHHmOgQAAGgFGhWaxo0b5/H817/+9RltBgAAoLVqVGhatGhRc/UBAADQqv1HF4IDAAD8tyA0AQAA2EBoAgAAsIHQBAAAYAOhCQAAwAZCEwAAgA2EJgAAABsITQAAADYQmgAAAGwgNAEAANhAaAIAALCB0AQAAGADoQkAAMAGQhMAAIANLRqannnmGfXr109BQUEKCgpSfHy8Vq1aZa03xmjGjBlyuVwKCAjQkCFDtG3bNo99VFdXa8qUKQoJCVFgYKBGjx6t4uJij5ry8nKlpKTI6XTK6XQqJSVFBw8e9KjZvXu3Ro0apcDAQIWEhCg9PV01NTXNdu4AAKBtadHQ1LVrVz322GP6+OOP9fHHH+vqq6/WddddZwWjOXPmaP78+VqwYIE2b96s8PBwDR8+XIcOHbL2kZGRoeXLlysnJ0f5+fk6fPiwkpKSVFdXZ9UkJyersLBQubm5ys3NVWFhoVJSUqz1dXV1GjlypCorK5Wfn6+cnBwtW7ZMmZmZZ28wAABAq+YwxpiWbuJ4nTp10ty5czVhwgS5XC5lZGTo3nvvlfTDrFJYWJhmz56tyZMny+12q3PnzlqyZInGjh0rSdqzZ48iIiK0cuVKJSYmaseOHYqOjtaGDRsUFxcnSdqwYYPi4+P1+eefKyoqSqtWrVJSUpKKiorkcrkkSTk5ORo/frzKysoUFBRkq/eKigo5nU653W7b29jV474VZ3R/x/v2sZHNtm8AAFo7u+/freaaprq6OuXk5KiyslLx8fHauXOnSktLlZCQYNX4+flp8ODBWrdunSSpoKBAtbW1HjUul0sxMTFWzfr16+V0Oq3AJEkDBgyQ0+n0qImJibECkyQlJiaqurpaBQUFp+y5urpaFRUVHg8AAHBuavHQ9Nlnn6l9+/by8/PTHXfcoeXLlys6OlqlpaWSpLCwMI/6sLAwa11paal8fX3VsWPH09aEhoY2OG5oaKhHzYnH6dixo3x9fa2ak5k1a5Z1nZTT6VREREQjzx4AALQVLR6aoqKiVFhYqA0bNug3v/mNxo0bp+3bt1vrHQ6HR70xpsGyE51Yc7L6ptScaNq0aXK73dajqKjotH0BAIC2q8VDk6+vry644AL1799fs2bN0sUXX6ynnnpK4eHhktRgpqesrMyaFQoPD1dNTY3Ky8tPW7N3794Gx923b59HzYnHKS8vV21tbYMZqOP5+flZd/4dewAAgHNTi4emExljVF1drZ49eyo8PFyrV6+21tXU1Gjt2rUaOHCgJCk2NlY+Pj4eNSUlJdq6datVEx8fL7fbrU2bNlk1GzdulNvt9qjZunWrSkpKrJq8vDz5+fkpNja2Wc8XAAC0Dd4tefD7779fI0aMUEREhA4dOqScnBx9+OGHys3NlcPhUEZGhrKzsxUZGanIyEhlZ2erXbt2Sk5OliQ5nU5NnDhRmZmZCg4OVqdOnZSVlaW+fftq2LBhkqQ+ffrommuuUWpqqhYuXChJmjRpkpKSkhQVFSVJSkhIUHR0tFJSUjR37lwdOHBAWVlZSk1NZfYIAABIauHQtHfvXqWkpKikpEROp1P9+vVTbm6uhg8fLkmaOnWqqqqqlJaWpvLycsXFxSkvL08dOnSw9vHEE0/I29tbY8aMUVVVlYYOHarFixfLy8vLqlm6dKnS09Otu+xGjx6tBQsWWOu9vLy0YsUKpaWladCgQQoICFBycrLmzZt3lkYCAAC0dq3ue5raMr6nCQCAtqfNfU8TAABAa0ZoAgAAsIHQBAAAYAOhCQAAwAZCEwAAgA2EJgAAABsITQAAADYQmgAAAGwgNAEAANhAaAIAALCB0AQAAGADoQkAAMAGQhMAAIANhCYAAAAbCE0AAAA2EJoAAABsIDQBAADYQGgCAACwgdAEAABgA6EJAADABkITAACADYQmAAAAGwhNAAAANhCaAAAAbCA0AQAA2EBoAgAAsIHQBAAAYAOhCQAAwAZCEwAAgA2EJgAAABsITQAAADa0aGiaNWuWfvazn6lDhw4KDQ3VL37xC33xxRceNcYYzZgxQy6XSwEBARoyZIi2bdvmUVNdXa0pU6YoJCREgYGBGj16tIqLiz1qysvLlZKSIqfTKafTqZSUFB08eNCjZvfu3Ro1apQCAwMVEhKi9PR01dTUNMu5AwCAtqVFQ9PatWt15513asOGDVq9erWOHj2qhIQEVVZWWjVz5szR/PnztWDBAm3evFnh4eEaPny4Dh06ZNVkZGRo+fLlysnJUX5+vg4fPqykpCTV1dVZNcnJySosLFRubq5yc3NVWFiolJQUa31dXZ1GjhypyspK5efnKycnR8uWLVNmZubZGQwAANCqOYwxpqWbOGbfvn0KDQ3V2rVrdeWVV8oYI5fLpYyMDN17772SfphVCgsL0+zZszV58mS53W517txZS5Ys0dixYyVJe/bsUUREhFauXKnExETt2LFD0dHR2rBhg+Li4iRJGzZsUHx8vD7//HNFRUVp1apVSkpKUlFRkVwulyQpJydH48ePV1lZmYKCgn60/4qKCjmdTrndblv1jdHjvhVndH/H+/axkc22bwAAWju779+t6pomt9stSerUqZMkaefOnSotLVVCQoJV4+fnp8GDB2vdunWSpIKCAtXW1nrUuFwuxcTEWDXr16+X0+m0ApMkDRgwQE6n06MmJibGCkySlJiYqOrqahUUFDTTGQMAgLbCu6UbOMYYo3vuuUc///nPFRMTI0kqLS2VJIWFhXnUhoWFadeuXVaNr6+vOnbs2KDm2PalpaUKDQ1tcMzQ0FCPmhOP07FjR/n6+lo1J6qurlZ1dbX1vKKiwvb5AgCAtqXVzDT99re/1aeffqpXX321wTqHw+Hx3BjTYNmJTqw5WX1Tao43a9Ys68Jyp9OpiIiI0/YEAADarlYRmqZMmaK33npLa9asUdeuXa3l4eHhktRgpqesrMyaFQoPD1dNTY3Ky8tPW7N3794Gx923b59HzYnHKS8vV21tbYMZqGOmTZsmt9ttPYqKihpz2gAAoA1p0dBkjNFvf/tbvfHGG/rggw/Us2dPj/U9e/ZUeHi4Vq9ebS2rqanR2rVrNXDgQElSbGysfHx8PGpKSkq0detWqyY+Pl5ut1ubNm2yajZu3Ci32+1Rs3XrVpWUlFg1eXl58vPzU2xs7En79/PzU1BQkMcDAACcm1r0mqY777xTr7zyiv7+97+rQ4cO1kyP0+lUQECAHA6HMjIylJ2drcjISEVGRio7O1vt2rVTcnKyVTtx4kRlZmYqODhYnTp1UlZWlvr27athw4ZJkvr06aNrrrlGqampWrhwoSRp0qRJSkpKUlRUlCQpISFB0dHRSklJ0dy5c3XgwAFlZWUpNTWVMAQAAFo2ND3zzDOSpCFDhngsX7RokcaPHy9Jmjp1qqqqqpSWlqby8nLFxcUpLy9PHTp0sOqfeOIJeXt7a8yYMaqqqtLQoUO1ePFieXl5WTVLly5Venq6dZfd6NGjtWDBAmu9l5eXVqxYobS0NA0aNEgBAQFKTk7WvHnzmunsAQBAW9KqvqepreN7mgAAaHva5Pc0AQAAtFaEJgAAABsITQAAADYQmgAAAGwgNAEAANhAaAIAALCB0AQAAGADoQkAAMAGQhMAAIANhCYAAAAbCE0AAAA2EJoAAABsIDQBAADYQGgCAACwgdAEAABgA6EJAADABkITAACADYQmAAAAGwhNAAAANhCaAAAAbCA0AQAA2EBoAgAAsIHQBAAAYAOhCQAAwAZCEwAAgA2EJgAAABsITQAAADYQmgAAAGwgNAEAANhAaAIAALCB0AQAAGADoQkAAMCGFg1N//u//6tRo0bJ5XLJ4XDozTff9FhvjNGMGTPkcrkUEBCgIUOGaNu2bR411dXVmjJlikJCQhQYGKjRo0eruLjYo6a8vFwpKSlyOp1yOp1KSUnRwYMHPWp2796tUaNGKTAwUCEhIUpPT1dNTU1znDYAAGiDWjQ0VVZW6uKLL9aCBQtOun7OnDmaP3++FixYoM2bNys8PFzDhw/XoUOHrJqMjAwtX75cOTk5ys/P1+HDh5WUlKS6ujqrJjk5WYWFhcrNzVVubq4KCwuVkpJira+rq9PIkSNVWVmp/Px85eTkaNmyZcrMzGy+kwcAAG2KwxhjWroJSXI4HFq+fLl+8YtfSPphlsnlcikjI0P33nuvpB9mlcLCwjR79mxNnjxZbrdbnTt31pIlSzR27FhJ0p49exQREaGVK1cqMTFRO3bsUHR0tDZs2KC4uDhJ0oYNGxQfH6/PP/9cUVFRWrVqlZKSklRUVCSXyyVJysnJ0fjx41VWVqagoCBb51BRUSGn0ym32217G7t63LfijO7veN8+NrLZ9g0AQGtn9/271V7TtHPnTpWWliohIcFa5ufnp8GDB2vdunWSpIKCAtXW1nrUuFwuxcTEWDXr16+X0+m0ApMkDRgwQE6n06MmJibGCkySlJiYqOrqahUUFDTreQIAgLbBu6UbOJXS0lJJUlhYmMfysLAw7dq1y6rx9fVVx44dG9Qc2760tFShoaEN9h8aGupRc+JxOnbsKF9fX6vmZKqrq1VdXW09r6iosHt6AACgjWm1M03HOBwOj+fGmAbLTnRizcnqm1JzolmzZlkXlzudTkVERJy2LwAA0Ha12tAUHh4uSQ1mesrKyqxZofDwcNXU1Ki8vPy0NXv37m2w/3379nnUnHic8vJy1dbWNpiBOt60adPkdrutR1FRUSPPEgAAtBWtNjT17NlT4eHhWr16tbWspqZGa9eu1cCBAyVJsbGx8vHx8agpKSnR1q1brZr4+Hi53W5t2rTJqtm4caPcbrdHzdatW1VSUmLV5OXlyc/PT7Gxsafs0c/PT0FBQR4PAABwbmrRa5oOHz6sf/3rX9bznTt3qrCwUJ06dVK3bt2UkZGh7OxsRUZGKjIyUtnZ2WrXrp2Sk5MlSU6nUxMnTlRmZqaCg4PVqVMnZWVlqW/fvho2bJgkqU+fPrrmmmuUmpqqhQsXSpImTZqkpKQkRUVFSZISEhIUHR2tlJQUzZ07VwcOHFBWVpZSU1MJQgAAQFILh6aPP/5YV111lfX8nnvukSSNGzdOixcv1tSpU1VVVaW0tDSVl5crLi5OeXl56tChg7XNE088IW9vb40ZM0ZVVVUaOnSoFi9eLC8vL6tm6dKlSk9Pt+6yGz16tMd3Q3l5eWnFihVKS0vToEGDFBAQoOTkZM2bN6+5hwAAALQRreZ7ms4FfE8TAABtT5v/niYAAIDWhNAEAABgA6EJAADABkITAACADYQmAAAAGwhNAAAANhCaAAAAbCA0AQAA2EBoAgAAsIHQBAAAYAOhCQAAwAZCEwAAgA2EJgAAABsITQAAADYQmgAAAGwgNAEAANhAaAIAALCB0AQAAGADoQkAAMAGQhMAAIANhCYAAAAbCE0AAAA2EJoAAABsIDQBAADYQGgCAACwgdAEAABgA6EJAADABkITAACADYQmAAAAGwhNAAAANhCaAAAAbCA0AQAA2EBoOsHTTz+tnj17yt/fX7Gxsfroo49auiUAANAKEJqO89prrykjI0MPPPCAtmzZoiuuuEIjRozQ7t27W7o1AADQwghNx5k/f74mTpyo22+/XX369NGTTz6piIgIPfPMMy3dGgAAaGGEpv9TU1OjgoICJSQkeCxPSEjQunXrWqgrAADQWni3dAOtxXfffae6ujqFhYV5LA8LC1NpaelJt6murlZ1dbX13O12S5IqKirOeH/11d+f8X0e0xz9AgDQVhx7HzTGnLaO0HQCh8Ph8dwY02DZMbNmzdLMmTMbLI+IiGiW3pqL88mW7gAAgJZ36NAhOZ3OU64nNP2fkJAQeXl5NZhVKisrazD7dMy0adN0zz33WM/r6+t14MABBQcHnzJoNUVFRYUiIiJUVFSkoKCgM7ZfeGKczx7G+uxgnM8OxvnsaM5xNsbo0KFDcrlcp60jNP0fX19fxcbGavXq1br++uut5atXr9Z111130m38/Pzk5+fnsez8889vth6DgoL4B3kWMM5nD2N9djDOZwfjfHY01zifbobpGELTce655x6lpKSof//+io+P13PPPafdu3frjjvuaOnWAABACyM0HWfs2LHav3+/Hn74YZWUlCgmJkYrV65U9+7dW7o1AADQwghNJ0hLS1NaWlpLt+HBz89P06dPb/BRIM4sxvnsYazPDsb57GCcz47WMM4O82P31wEAAIAvtwQAALCD0AQAAGADoQkAAMAGQhMAAIANhKZW4umnn1bPnj3l7++v2NhYffTRR6etX7t2rWJjY+Xv769evXrp2WefPUudtm2NGec33nhDw4cPV+fOnRUUFKT4+Hi9++67Z7Hbtquxr+dj/vGPf8jb21uXXHJJ8zZ4DmnsWFdXV+uBBx5Q9+7d5efnp5/+9Kf6y1/+cpa6bbsaO85Lly7VxRdfrHbt2qlLly667bbbtH///rPUbdv0v//7vxo1apRcLpccDofefPPNH93mrL8XGrS4nJwc4+PjY55//nmzfft2c9ddd5nAwECza9euk9Z/8803pl27duauu+4y27dvN88//7zx8fExr7/++lnuvG1p7DjfddddZvbs2WbTpk3myy+/NNOmTTM+Pj7mk08+Ocudty2NHedjDh48aHr16mUSEhLMxRdffHaabeOaMtajR482cXFxZvXq1Wbnzp1m48aN5h//+MdZ7Lrtaew4f/TRR+a8884zTz31lPnmm2/MRx99ZC666CLzi1/84ix33rasXLnSPPDAA2bZsmVGklm+fPlp61vivZDQ1Apcfvnl5o477vBYduGFF5r77rvvpPVTp041F154oceyyZMnmwEDBjRbj+eCxo7zyURHR5uZM2ee6dbOKU0d57Fjx5rf//73Zvr06YQmmxo71qtWrTJOp9Ps37//bLR3zmjsOM+dO9f06tXLY9kf//hH07Vr12br8VxjJzS1xHshH8+1sJqaGhUUFCghIcFjeUJCgtatW3fSbdavX9+gPjExUR9//LFqa2ubrde2rCnjfKL6+nodOnRInTp1ao4WzwlNHedFixbp66+/1vTp05u7xXNGU8b6rbfeUv/+/TVnzhz95Cc/Ue/evZWVlaWqqqqz0XKb1JRxHjhwoIqLi7Vy5UoZY7R37169/vrrGjly5Nlo+b9GS7wX8o3gLey7775TXV2dwsLCPJaHhYWptLT0pNuUlpaetP7o0aP67rvv1KVLl2brt61qyjif6PHHH1dlZaXGjBnTHC2eE5oyzl999ZXuu+8+ffTRR/L25j9JdjVlrL/55hvl5+fL399fy5cv13fffae0tDQdOHCA65pOoSnjPHDgQC1dulRjx47VkSNHdPToUY0ePVp/+tOfzkbL/zVa4r2QmaZWwuFweDw3xjRY9mP1J1sOT40d52NeffVVzZgxQ6+99ppCQ0Obq71zht1xrqurU3JysmbOnKnevXufrfbOKY15TdfX18vhcGjp0qW6/PLLde2112r+/PlavHgxs00/ojHjvH37dqWnp+uhhx5SQUGBcnNztXPnTn78vRmc7fdC/m9dCwsJCZGXl1eD/8dSVlbWIEEfEx4eftJ6b29vBQcHN1uvbVlTxvmY1157TRMnTtTf/vY3DRs2rDnbbPMaO86HDh3Sxx9/rC1btui3v/2tpB/e2I0x8vb2Vl5enq6++uqz0ntb05TXdJcuXfSTn/xETqfTWtanTx8ZY1RcXKzIyMhm7bktaso4z5o1S4MGDdLvfvc7SVK/fv0UGBioK664Qo888gifBpwhLfFeyExTC/P19VVsbKxWr17tsXz16tUaOHDgSbeJj49vUJ+Xl6f+/fvLx8en2Xpty5oyztIPM0zjx4/XK6+8wvUINjR2nIOCgvTZZ5+psLDQetxxxx2KiopSYWGh4uLizlbrbU5TXtODBg3Snj17dPjwYWvZl19+qfPOO09du3Zt1n7bqqaM8/fff6/zzvN8e/Xy8pL0/2dC8J9rkffCZrvEHLYdu531hRdeMNu3bzcZGRkmMDDQfPvtt8YYY+677z6TkpJi1R+7zfLuu+8227dvNy+88AJfOWBDY8f5lVdeMd7e3uZ//ud/TElJifU4ePBgS51Cm9DYcT4Rd8/Z19ixPnTokOnatau54YYbzLZt28zatWtNZGSkuf3221vqFNqExo7zokWLjLe3t3n66afN119/bfLz803//v3N5Zdf3lKn0CYcOnTIbNmyxWzZssVIMvPnzzdbtmyxvtqhNbwXEppaif/5n/8x3bt3N76+vuayyy4za9eutdaNGzfODB482KP+ww8/NJdeeqnx9fU1PXr0MM8888xZ7rhtasw4Dx482Ehq8Bg3btzZb7yNaezr+XiEpsZp7Fjv2LHDDBs2zAQEBJiuXbuae+65x3z//fdnueu2p7Hj/Mc//tFER0ebgIAA06VLF3PLLbeY4uLis9x127JmzZrT/je3NbwXOoxhrhAAAODHcE0TAACADYQmAAAAGwhNAAAANhCaAAAAbCA0AQAA2EBoAgAAsIHQBAAAYAOhCQAAwAZCE4BzXllZmSZPnqxu3brJz89P4eHhSkxM1Pr161u6NQBtiHdLNwAAze1Xv/qVamtr9eKLL6pXr17au3ev3n//fR04cKBZjldTUyNfX99m2TeAlsNME4Bz2sGDB5Wfn6/Zs2frqquuUvfu3XX55Zdr2rRpGjlypFUzadIkhYWFyd/fXzExMXrnnXesfSxbtkwXXXSR/Pz81KNHDz3++OMex+jRo4ceeeQRjR8/Xk6nU6mpqZKkdevW6corr1RAQIAiIiKUnp6uysrKs3fyAM4oQhOAc1r79u3Vvn17vfnmm6qurm6wvr6+XiNGjNC6dev08ssva/v27Xrsscfk5eUlSSooKNCYMWN000036bPPPtOMGTP04IMPavHixR77mTt3rmJiYlRQUKAHH3xQn332mRITE/XLX/5Sn376qV577TXl5+frt7/97dk4bQDNgB/sBXDOW7ZsmVJTU1VVVaXLLrtMgwcP1k033aR+/fopLy9PI0aM0I4dO9S7d+8G295yyy3at2+f8vLyrGVTp07VihUrtG3bNkk/zDRdeumlWr58uVVz6623KiAgQAsXLrSW5efna/DgwaqsrJS/v38znjGA5sBME4Bz3q9+9Svt2bNHb731lhITE/Xhhx/qsssu0+LFi1VYWKiuXbueNDBJ0o4dOzRo0CCPZYMGDdJXX32luro6a1n//v09agoKCrR48WJrpqt9+/ZKTExUfX29du7ceeZPEkCz40JwAP8V/P39NXz4cA0fPlwPPfSQbr/9dk2fPl1ZWVmn3c4YI4fD0WDZiQIDAz2e19fXa/LkyUpPT29Q261btyacAYCWRmgC8F8pOjpab775pvr166fi4mJ9+eWXJ51tio6OVn5+vseydevWqXfv3tZ1Tydz2WWXadu2bbrgggvOeO8AWgYfzwE4p+3fv19XX321Xn75ZX366afauXOn/va3v2nOnDm67rrrNHjwYF155ZX61a9+pdWrV2vnzp1atWqVcnNzJUmZmZl6//339Yc//EFffvmlXnzxRS1YsOBHZ6juvfderV+/XnfeeacKCwv11Vdf6a233tKUKVPOxmkDaAbMNAE4p7Vv315xcXF64okn9PXXX6u2tlYRERFKTU3V/fffL+mHC8WzsrJ08803q7KyUhdccIEee+wxST/MGP31r3/VQw89pD/84Q/q0qWLHn74YY0fP/60x+3Xr5/Wrl2rBx54QFdccYWMMfrpT3+qsWPHNvcpA2gm3D0HAABgAx/PAQAA2EBoAgAAsIHQBAAAYAOhCQAAwAZCEwAAgA2EJgAAABsITQAAADYQmgAAAGwgNAEAANhAaAIAALCB0AQAAGADoQkAAMCG/wcI76Fv4oMQMAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_logits_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    images, _ = next(iter(valid_loader))\n",
    "    images = images.to(DEVICE)\n",
    "    _, conf_logits = model(images)\n",
    "    conf_logits_list.append(conf_logits)\n",
    "    conf_scores = torch.sigmoid(conf_logits)\n",
    "\n",
    "conf_scores = conf_scores.cpu().flatten().numpy()\n",
    "print(conf_scores)\n",
    "plt.hist(conf_scores, bins=20, range=(0, 1))\n",
    "plt.title(\"Confidence Score Distribution\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6cced64-8c91-49a5-96c7-7e2bafb9b7c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T15:04:16.064801Z",
     "iopub.status.busy": "2025-08-06T15:04:16.064498Z",
     "iopub.status.idle": "2025-08-06T15:04:16.226316Z",
     "shell.execute_reply": "2025-08-06T15:04:16.225769Z",
     "shell.execute_reply.started": "2025-08-06T15:04:16.064779Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "Image shape: torch.Size([3, 256, 256])\n",
      " GT boxes: torch.Size([1, 4])\n",
      "Pred boxes+conf: 8192 x 5\n",
      "\n",
      "torch.Size([8192, 4])\n",
      "1.0\n",
      "Image shape: torch.Size([3, 256, 256])\n",
      " GT boxes: torch.Size([1, 4])\n",
      "Pred boxes+conf: 8192 x 5\n",
      "\n",
      "torch.Size([8192, 4])\n",
      "1.0\n",
      "Image shape: torch.Size([3, 256, 256])\n",
      " GT boxes: torch.Size([1, 4])\n",
      "Pred boxes+conf: 8192 x 5\n",
      "\n",
      "torch.Size([8192, 4])\n",
      "1.0\n",
      "Image shape: torch.Size([3, 256, 256])\n",
      " GT boxes: torch.Size([1, 4])\n",
      "Pred boxes+conf: 8192 x 5\n",
      "\n",
      "torch.Size([8192, 4])\n",
      "1.0\n",
      "Image shape: torch.Size([3, 256, 256])\n",
      " GT boxes: torch.Size([1, 4])\n",
      "Pred boxes+conf: 8192 x 5\n",
      "\n",
      "torch.Size([8192, 4])\n",
      "1.0\n",
      "Image shape: torch.Size([3, 256, 256])\n",
      " GT boxes: torch.Size([1, 4])\n",
      "Pred boxes+conf: 8192 x 5\n",
      "\n",
      "torch.Size([8192, 4])\n",
      "1.0\n",
      "Image shape: torch.Size([3, 256, 256])\n",
      " GT boxes: torch.Size([1, 4])\n",
      "Pred boxes+conf: 8192 x 5\n",
      "\n",
      "torch.Size([8192, 4])\n",
      "1.0\n",
      "Image shape: torch.Size([3, 256, 256])\n",
      " GT boxes: torch.Size([1, 4])\n",
      "Pred boxes+conf: 8192 x 5\n",
      "\n",
      "torch.Size([8192, 4])\n",
      "1.0\n",
      "Image shape: torch.Size([3, 256, 256])\n",
      " GT boxes: torch.Size([1, 4])\n",
      "Pred boxes+conf: 8192 x 5\n",
      "\n",
      "torch.Size([8192, 4])\n",
      "1.0\n",
      "Image shape: torch.Size([3, 256, 256])\n",
      " GT boxes: torch.Size([1, 4])\n",
      "Pred boxes+conf: 8192 x 5\n",
      "\n",
      "torch.Size([8192, 4])\n"
     ]
    }
   ],
   "source": [
    "num_images = 10\n",
    "init_image = random.randint(0, len(test_loader))\n",
    "for batch in train_loader:\n",
    "    inputs, labels = batch\n",
    "    inputs = inputs.to(DEVICE)\n",
    "\n",
    "    # Forward whole batch at once\n",
    "    with torch.no_grad():\n",
    "        all_coords, all_confs = model(inputs)\n",
    "        all_confs = torch.sigmoid(all_confs)\n",
    "\n",
    "    # Loop over images in batch\n",
    "    for img, l_gt, s_gt, coords, confs in zip(inputs[init_image:init_image+num_images], labels['l_boxes'][init_image:init_image+num_images], labels['s_boxes'][init_image:init_image+num_images], all_coords[init_image:init_image+num_images], all_confs[init_image:init_image+num_images]):\n",
    "        # Move to CPU / numpy for printing & plotting\n",
    "        img = img.to(DEVICE)\n",
    "        H, W = img.shape[1:]\n",
    "        l_gt = l_gt.to(DEVICE)\n",
    "        s_gt = s_gt.to(DEVICE)\n",
    "\n",
    "        coords = coords.to(DEVICE)   # list of [x1,y1,x2,y2]\n",
    "        coords_px = coords\n",
    "        coords_px = coords_px.tolist()\n",
    "        print((coords_px[0][2]-coords_px[0][0])/(coords_px[0][3]-coords_px[0][1]))\n",
    "\n",
    "        confs = confs.to(DEVICE).tolist()    # list of floats in [0,1]\n",
    "\n",
    "        print(f\"Image shape: {img.shape}\")\n",
    "        print(f\" GT boxes: {l_gt.shape}\")\n",
    "        print(f\"Pred boxes+conf: {len(coords)} x 5\\n\")\n",
    "\n",
    "        # Print each box + its confidence\n",
    "        print(coords.shape)\n",
    "        # for (x1,y1,x2,y2), c in zip(coords, confs):\n",
    "        #     print(f\"  → Box: [{x1:.4f},{y1:.4f},{x2:.4f},{y2:.4f}]  conf={c:.4f}\")\n",
    "        \n",
    "        # print()\n",
    "\n",
    "        # Visualize (with confidences overlaid)\n",
    "        # show_boxes_with_confidence(img, l_gt_boxes=l_gt, s_gt_boxes=s_gt, pred_boxes=coords, confidences=confs)\n",
    "\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
